## baby-llama2-chinese-fix
å‚è€ƒhttps://github.com/DLLXW/baby-llama2-chineseï¼šç”¨äºä»å¤´é¢„è®­ç»ƒ+SFTä¸€ä¸ªå°å‚æ•°é‡çš„ä¸­æ–‡LLaMa2çš„ä»“åº“ï¼›24Gå•å¡å³å¯è¿è¡Œå¾—åˆ°ä¸€ä¸ªæµç•…ä¸­æ–‡é—®ç­”çš„chat-llama2.

æœ¬é¡¹ç›®æ˜¯ä¾¿äºè‡ªå·±å­¦ä¹ LLMç›¸å…³çŸ¥è¯†æ‰€å»ºï¼Œå®ç°äº†ä¸€äº›åŠŸèƒ½ï¼Œä½†æ²¡æœ‰è¯¦ç»†çš„æµ‹è¯•ï¼Œä»£ç ä¸­éš¾å…å­˜åœ¨ä¸€äº›bugã€‚


#ã€‚ã€‚ã€‚è®­ç»ƒä»£ç ç›®å‰æœ‰bugï¼Œç”±äºç²¾åŠ›æœ‰é™ï¼Œæš‚æ—¶è¿˜æœªä¿®å¤ã€‚ã€‚ã€‚ğŸ˜­


## æ›´æ–°è®°å½•

>2024.03.20ï¼šæ”¯æŒGPTQé‡åŒ–ï¼Œå¯ä»¥è¿è¡Œã€‚å¢åŠ llm.int8/awq/onebité‡åŒ–ä»£ç ï¼Œä½†ä»£ç æœªæµ‹è¯•ï¼Œ[https://zhuanlan.zhihu.com/p/684907262]

>2024.03.10ï¼šå¢åŠ YaRN/CLEXç­‰ä½ç½®ç¼–ç ï¼Œè§£å†³kv_cacheçš„bugã€‚https://zhuanlan.zhihu.com/p/684907262

>2024.03.02ï¼šæ”¯æŒLoRAè®­ç»ƒï¼Œæ ¹æ®LongLoRAä¼˜åŒ–ä»£ç ï¼Œæ”¯æŒSS-Attn

>2024.02.29ï¼šæ”¯æŒé•¿åº¦å¤–æ¨ï¼Œfrom LLaMAã€‚ https://zhuanlan.zhihu.com/p/683731440

>2024.02.24ï¼šæ”¯æŒdeepspeedè®­ç»ƒã€‚https://zhuanlan.zhihu.com/p/683768690

>2023.11.02ï¼šå¢åŠ è®­ç»ƒtokenizerä»£ç ï¼Œæ‰©å±•æ•°æ®ã€‚https://zhuanlan.zhihu.com/p/664046612

>2023.10.21ï¼šæµ‹è¯•falsh attention

>2023.10.13ï¼šforkä»£ç ï¼Œè®­ç»ƒå®æˆ˜ã€‚https://zhuanlan.zhihu.com/p/660759033





## è®­ç»ƒæ•°æ®
- Wikiä¸­æ–‡ç™¾ç§‘ï¼ˆ25wè¯æ¡ï¼‰[wikipedia-cn-20230720-filtered](https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered)
- BaiduBaiKeï¼ˆ563wè¯æ¡ï¼‰
[ç™¾åº¦ç½‘ç›˜](https://pan.baidu.com/s/1jIpCHnWLTNYabftavo3DVw?pwd=bwvb)
 æå–ç : bwvb
- [Medical Dataset](https://huggingface.co/datasets/shibing624/medical/tree/main)

é™¤æ­¤ä¹‹å¤–ï¼Œä¸ºäº†è®©æ¨¡å‹å…·å¤‡åœ¨æŸä¸€ä¸ªä¸“æœ‰é¢†åŸŸçš„èƒ½åŠ›ï¼Œè¿™é‡Œé€‰ç”¨äº†â€œåŒ»ç–—é—®ç­”â€ä½œä¸ºåˆ‡å…¥ç‚¹ï¼Œå°è¯•æ”¶é›†äº†å¾ˆå¤šçš„åŒ»ç–—æ•°æ®å’Œä¸Šé¢çš„é€šç”¨è¯­æ–™ä¸€èµ·å–‚ç»™æ¨¡å‹ã€‚

tipsï¼šè®­ç»ƒæ•°æ®åœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­æœ‰æ‰€æ‰©å±•ï¼Œè¯¦æƒ…å¯å‚è€ƒçŸ¥ä¹æ–‡ç« ã€‚


## ä¸­æ–‡åˆ†è¯å™¨

é‡‡ç”¨ChatGLM2çš„åˆ†è¯å™¨ã€‚

## é¢„è®­ç»ƒè¯­æ–™é¢„å¤„ç†
```python
#è„šæœ¬é‡Œé¢æ¯ä¸€ä¸ªå‡½æ•°å¯¹åº”ä¸€ä¸ªè¯­æ–™åº“çš„é¢„å¤„ç†ï¼Œæ­å»ºæ–°åŠ è¯­æ–™å¯ä»¥è‡ªè¡Œæ‰©å±•ã€‚
python data_process.py
#è¿è¡Œç»“æŸåï¼Œä¼šåœ¨./dataç›®å½•ä¸‹äº§ç”Ÿ.binæ–‡ä»¶
```
æ•°æ®é¢„å¤„ç†é‡‡å–GPTçš„é€šç”¨åšæ³•ï¼Œå¯¹è¯­æ–™è¿›è¡Œæå‰åˆ†è¯ï¼Œå¯¹ä¸€ä¸ªæ ·æœ¬åšå®Œåˆ†è¯ååœ¨æœ«å°¾åŠ ä¸Šä¸€ä¸ªç»“æŸç¬¦å·ï¼Œä¸ä¸‹ä¸€ä¸ªæ ·æœ¬åŒºåˆ†å¼€ã€‚ç„¶åå°†æ‰€æœ‰çš„è®­ç»ƒè¯­æ–™æ‹¼æ¥æˆä¸€ä¸ªæ•°ç»„ï¼ˆnp.uint16ï¼‰ä»¥.binäºŒè¿›åˆ¶æ ¼å¼å­˜å‚¨åˆ°ç£ç›˜ä¸Šã€‚å¦‚æœè¯­æ–™è¿‡å¤§ï¼Œé¿å…å†…å­˜æº¢å‡ºï¼Œå¯ä»¥é€‰æ‹©mmapæ ¼å¼ã€‚

## è®­ç»ƒè‡ªå·±çš„åˆ†è¯å™¨
å¦‚æœè¦é‡æ–°è®­ç»ƒè‡ªå·±çš„åˆ†è¯å™¨ï¼Œå¯ä»¥åœ¨data_process.pyä»£ç ä¸­è®¾ç½®save_all_textä¸ºTrueï¼Œä¼šå°†æ–‡æœ¬ä¿¡æ¯æ±‡æ€»ä¿å­˜åˆ°æœ¬åœ°txtï¼Œç„¶åå¯ä»¥è®­ç»ƒè‡ªå·±çš„åˆ†è¯å™¨ã€‚
```python
#è·å–dataæ–‡ä»¶å¤¹ä¸‹çš„å…¨éƒ¨tokenizer_xxx.txtæ–‡ä»¶ã€‚
python train_tokenizer.py
#è¿è¡Œç»“æŸåï¼Œä¼šåœ¨å½“å‰ç›®å½•ä¸‹äº§ç”Ÿtokenizer.modelæ–‡ä»¶
```

ä»£ç æ¥è‡ªï¼šhttps://github.com/yanqiangmiffy/how-to-train-tokenizer

## SFTæ ·æœ¬æ„å»º
ä¸­æ–‡SFTè¯­æ–™æœ€è¿‘é™†é™†ç»­ç»­å¼€æºäº†å¾ˆå¤šï¼ˆ[bell](https://huggingface.co/datasets/BelleGroup/train_1M_CN)ã€[MOSS](https://github.com/OpenLMLab/MOSS/tree/main/SFT_data)ã€[alpaca-zh](https://huggingface.co/datasets/shibing624/alpaca-zh)ç­‰ï¼‰ï¼Œä½†æ˜¯å¦ç™½è®²ï¼Œè´¨é‡éƒ½ä¸é«˜ï¼Œå¤§å®¶å¯è‡ªè¡Œä¸‹è½½å¹¶éœ€è¦è¿›è¡Œæ¸…æ´—ï¼Œæ¸…æ´—SFTæ•°æ®æ˜¯ä¸ªè€—æ—¶è€—åŠ›çš„å·¥ä½œã€‚
ä¸­æ–‡SFTè¯­æ–™ç½‘ä¸Šæœ€è¿‘å¾ˆå¤šï¼Œå¤§å®¶è‡ªè¡Œä¸‹è½½ã€‚å‚è€ƒdataset_sft.pyå³å¯ï¼

åŸºæœ¬é€»è¾‘å¦‚ä¸‹ï¼š
- promptå’Œanswerä¹‹é—´ä¸€å®šè¦æœ‰ä¸€ä¸ªå¼€å§‹ç¬¦éš”å¼€ï¼Œç„¶åansweråéœ€è¦ä¸€ä¸ªç»“æŸç¬¦ã€‚
- è®¡ç®—lossçš„æ—¶å€™ï¼Œå¯¹promptéƒ¨åˆ†çš„lossè¿›è¡Œmaskï¼Œåªè®¡ç®—answeréƒ¨åˆ†çš„losså³å¯ã€‚

## é¢„è®­ç»ƒ+SFT
å‚è€ƒrun.sh

```python
# python data_prepare.py
# CUDA_VISIBLE_DEVICES=1 python data_prepare.py

# é‡æ–°è®­ç»ƒtokenizer
# CUDA_VISIBLE_DEVICES=1 python train_tokenizer.py

use_accelerate=false
use_nohup=false

if [ use_accelerate == true ] ; then
    echo "[LLM] use accelerate"
    if [ use_nohup == true ] ; then
        echo "[LLM] use nohup"
        CUDA_VISIBLE_DEVICES=1,2,3,4 nohup python -m torch.distributed.launch --nproc_per_node=8 --use_env pretrain.py >out/pretrain_1_log
        CUDA_VISIBLE_DEVICES=1,2,3,4 nohup python -m torch.distributed.launch --nproc_per_node=8 --use_env fine_tuning.py >out/fine_tuning_log
        CUDA_VISIBLE_DEVICES=0 nohup python eval.py >out/eval_log
    else
        CUDA_VISIBLE_DEVICES=0,1,5,6 python -m torch.distributed.launch --nproc_per_node=4 --use_env pretrain.py
        CUDA_VISIBLE_DEVICES=0,1,5,6 python -m torch.distributed.launch --nproc_per_node=4 --use_env fine_tuning.py
        CUDA_VISIBLE_DEVICES=1 python eval.py
    fi
else  # deepspeed
    echo "[LLM] use deepspeed"
    if [ use_nohup == true ] ; then
        echo "[LLM] use nohup"
        CUDA_VISIBLE_DEVICES=1,2,3,4 nohup deepspeed --num_gpus=4 pretrain.py  --use_deepspeed True >out/pretrain_ds_log
        CUDA_VISIBLE_DEVICES=1,2,3,4 nohup deepspeed --num_gpus=4 fine_tuning.py  --use_deepspeed True >out/fine_tuning_ds_log
        CUDA_VISIBLE_DEVICES=10 nohup python eval.py >out/eval_ds_log
    else
        CUDA_VISIBLE_DEVICES=0,1,5,6 deepspeed --num_gpus=4 pretrain.py --use_deepspeed True
        CUDA_VISIBLE_DEVICES=0,1,5,6 deepspeed --num_gpus=4 fine_tuning.py --use_deepspeed True
        CUDA_VISIBLE_DEVICES=1 python eval.py
    fi
fi

```


æ ¹æ®è‡ªå·±ç®—åŠ›çš„æƒ…å†µåˆç†çš„è°ƒèŠ‚ä»¥ä¸‹å‚æ•°ï¼Œæ§åˆ¶æ¨¡å‹çš„è®¡ç®—é‡å’Œå‚æ•°é‡ï¼Œè¿™æ˜¯ç¬¬ä¸€ç‰ˆä½¿ç”¨çš„å‚æ•°
- max_seq_len = 256
- dim = 512
- n_layers = 8
- n_heads = 8

æ¨ç†è„šæœ¬å¯ä»¥å‚è€ƒeval.pyã€‚
