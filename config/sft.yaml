out_dir : 'out'
train_data_path: [
        './data/pretrain_data.bin'
    ]
sft_data_path: './data/sft_data.csv'
test_data_path: [
        './data/test_en_1.json',
        './data/test_encyclopedia.json',
        './data/test_zh_0.json',
        './data/test.json',
    ]
log_interval : 100
save_interval : 100
always_save_checkpoint : True # if True, always save a checkpoint after each eval
max_epoch : 1

batch_size : 16  # if gradient_accumulation_steps > 1, this is the micro-batch size
grad_accum_steps : 16 # used to simulate larger batch sizes
use_profile: false

sft_params:
  type: 'fft'  # fft/lora/dora/lisa
  only_save_lora: false
  max_epoch : 2
  # adamw optimizer
  lr : 0.00002 # max learning rate
  weight_decay : 0.0001
  beta1 : 0.9
  beta2 : 0.95
  grad_clip : 1.0 # clip gradients at this value, or disable if :: 0.0

  # lisa
  interval_steps : 16 # 切换激活层的step
  act_layers : 1 # 激活的层数

  # learning rate decay settings
  decay_lr : True # whether to decay the learning rate
  warmup_iters : 1000 # how many steps to warm up for
  lr_decay_iters : 50000 # should be ~: max_iters per Chinchilla
  min_lr : 0.000001 # minimum learning rate, should be ~: learning_rate/10 per Chinchilla

# system
device : 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks
dtype : 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler
compile : True # use PyTorch 2.0 to compile the model to be faster

gen_params:
  max_new_tokens: 256
  temperature: 0.8
  top_k: 20
  shot: 1